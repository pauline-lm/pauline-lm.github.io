---
layout: archive
title: "Reading"
permalink: /reading/
author_profile: true
---


*Luke S.G. (2017) [Evaluating significance in linear mixed-effects models in R.](https://pubmed.ncbi.nlm.nih.gov/27620283/){:targets="_blank"} Behavioral Research Methods, 49(4):1494-1502. doi: 10.3758/s13428-016-0809-y.*

Evaluating significance of fixed effects in mixed-effect models is tricky. One often sees likelihood ratio test (LRT, comparison of fit of two models) or Wald tests (based on distribution). Here, Steven G Luke reports the Type 1 error rate for different simulations (changing the number of subjects and items) and different methods (the two usual ones and alternatives). In short: nothing is perfect. Also, it is good to remember that Type 1 error is sensitive to both the number of subjects and items. In other words, we cannot make up for a small number of subjects but having many items (and vice versa). This short article nicely explains why there are good reasons that p-values are not included by default in packages like lme4 and that one has to actually think instead of reporting blindly p-values.

For more general thoughts on statistical methods used in psychological science, the work of [Geoff Cumming](https://scholar.google.com/citations?user=0YcrKmYAAAAJ&hl=en){:target="_blank”}, such as [The New Statistics: Why and How](https://journals.sagepub.com/doi/10.1177/0956797613504966){:target="_blank”} or these [great videos](https://www.youtube.com/user/geoffdcumming/videos){:target="_blank”} is an eye opener!

***

*Snow, D., Balog, H.L. (2002) [Do children produce the melody before the words? A review of developmental intonation research](https://www.sciencedirect.com/science/article/pii/S0024384102000608?via%3Dihub){:target="_blank”}, Lingua, 112(12), 1025-1058. doi: 10.1016/S0024-3841(02)00060-8*

The authors review literature (published before 2002, obviously) on the expressive intonation of children during the first two years of life. Build on the work of giants of the field such as Bolinger (1986), Cooper and Sorensen (1981), ’t Hart (1981) or ’t Hart et al. (1990), they first define the notions of intonation groups, nuclear tones, tonal features, and tonal meanings, before reporting studies on the development of intonation itself. In (very) short: most literature suggests the use of precursors of intonation from the age 0;3 to 0;9, followed by a regression (at the beginning of a period of prosodic reorganisation that lasts until the end of the single-word period or longer), then developing again (with intentional communication). I am looking forward to read how the more recent literature (research using acoustic descriptors as well as cross-cultural and cross-domain approaches) supports (or not) the theoretical thoughts of the authors.

**Update on the development topic** 

*Wermke, K., Robb, M.P. & Schluter, P.J. (2021) [Melody complexity of infants’ cry and non-cry vocalisations increases across the first six months.](https://www.nature.com/articles/s41598-021-83564-8){:target="_blank”}, Scientific Reports 11, 4137. doi: 10.1038/s41598-021-83564-8*

The authors examined more than 67,500 (!) cry and non-cry vocalisations of babies up to 6 months old. First, they discarded sounds without patterns and then counted the number of arcs (i.e., melodic line longer than 150ms, going up and down, with a frequency amplitude of at least three (for cry) or two (for non-cry) semitones). They observe an increase in complexity, from single arcs to multiple ones in a single vocalisation, in both cry and non-cry vocalisation across the first 180 days. They also see a slight complexity reduction in the non-cry vocalisations at about 140 days, which is earlier than what report [Snow and Balog (2002) in their review](https://www.sciencedirect.com/science/article/pii/S0024384102000608?via%3Dihub){:target="_blank”} but which confirms that the development is not linear. Kathleen Wermke and her co-authors discuss the role of neural, motor, and perceptual development in the complexification of vocalisations. 

That is nice to see such a systematic approach and it is very tempting to analyse other acoustic aspects of this material (if this is not already done :)).

**Update on the development topic** 

*Frota, S., & Butler, J. (2018). [Early development of intonation](https://labfon.letras.ulisboa.pt/texts/Frota-Butler_2018.pdf){:target="_blank”}, In The Development of Prosody in First Language Acquisition, Eds. Pilar Prieto and Núria Esteve-Gilbert, pp. 145-164.* 

The authors describe studies on perception and production of intonation that use the Autosegmental Metrical framework. For the perception side, they remind us that infants are sensitive to intonation of their ambient language from 4-5 months old and linguistically use cues from 6 months old. For the production side (i.e., less studied than the perception side), studies show a precocious development of the intonational system of the ambient language (in line with what was already reported in the review of [Snow and Balog (2002)](https://www.sciencedirect.com/science/article/pii/S0024384102000608?via%3Dihub){:target="_blank”}, and that phonological patterns are in place before the phonetics of tonal alignment and tonal scaling. The direct comparison of perception/production, the parallel with other domains, and the focus on atypical populations (see the interesting work of [Sue Peppé](https://www.tandfonline.com/doi/abs/10.1080/17549500902906339){:target="_blank”} in the same book and elsewhere) are promising to fully get how intonation develops!

**Update on the development topic** 

*Esteve-Gilbert, N., & Prieto, P. (2018). [Early development of the prosody-meaning interface](https://benjamins.com/catalog/tilar.23.12est){:target="_blank”}, In The Development of Prosody in First Language Acquisition, Eds. Pilar Prieto and Núria Esteve-Gilbert, pp. 227-246.* 

The authors summarise findings on the mapping of prosodic cues to social meaning, which is an important ability to understand communicative partners (!). The authors remind us of the existence of intonation patterns (in production and perception) but, more interestingly, give an overview of what children “use to communicate”. For instance, children of 2 months old already make strong (acoustic) contrast between their positive and negative vocalisations, at 5 months old, they adapt their vocalisations to influence the environment, between 7 and 12 months old, a new area opens together with the joint attention abilities, and development continues, independently of grammatical development and before the emergence of 2-words combination. It is of course important to describe intonation development, like [Snow & Balog, 2002](https://www.sciencedirect.com/science/article/pii/S0024384102000608?via%3Dihub){:target="_blank”} or [Frota & Butler, 2018](https://labfon.letras.ulisboa.pt/texts/Frota-Butler_2018.pdf){:target="_blank”}, but this chapter add the “meaning” side, which gives a larger picture on this topic.

***

*Heintz, C., & Scott-Phillips, T. (2022). [Expression unleashed: The evolutionary & cognitive foundations of human communication.](https://pubmed.ncbi.nlm.nih.gov/34983701/){:target="_blank"} Behavioral and Brain Sciences, 1-46. doi:10.1017/S0140525X22000012*

This target article defends the claim that the diversity of human expression is associated with an ensemble of cognitive capacities targeted at the expression/recognition of informative intentions. Grounded on the work of Grice (and many more since then) on cognitive pragmatics, the authors describe what is informative and communicative intentions through simple examples (see also this [TED talk](https://www.ted.com/talks/thom_scott_phillips_how_communication_makes_us_human){:target="_blank”}) and define different types of inferences made by the audience: inferences about 1) others’ intentions, 2) others’ informative intentions, and 3) others’ communicative intentions. Whereas ostensive communication is not limited to language, it is a good reminder about its complexity. For a great introduction, read the inspiring book [Speaking our minds](https://thomscottphillips.com/book/){:target="_blank”} written by Thom Scott-Phillips.

Definitely looking forward to read the comments about the proposed cognitive capacities and their co-evolution!

***

*Wojcik, E.H., Lassman, D.J., & Vuvan, D.T. (2022). [Using a developmental-ecological approach to understand the relation between language and music.](https://www.frontiersin.org/articles/10.3389/fpsyg.2022.762018/full){:targets="_blank"} Frontiers in Psychology. 13:762018. doi: 10.3389/fpsyg.2022.762018* 

This short perspective article gives arguments to use a developmental-ecological approach in music-language studies. It has been repeatedly shown that implicit learning is key in development but what is the “real” input that we receive when growing up? Inspired by research in other fields (e.g., motor and semantic development), the authors defend that describing input in real life (and how it unfolds over time) is a good idea to document listeners experience (and understand better adult cognition). Of course, there are challenges (e.g., how can we code the input? is it music? language? do we even agree on that?) but it is a good reminder that lab experiments are limited and that “field” experiments in this domain could be helpful.

***

*Deliège, I. (1987) [Grouping conditions in listening to music: An approach to Lerdahl & Jackendoff's grouping preference rules.](https://psycnet.apa.org/record/1988-31599-001){:targets="_blank"}  Music Perception, 4(4):325–359. doi:10.2307/40285378.*

In relation to Gestalt Theory, Lerdahl and Jackendoff (1981, 1983) formulated grouping preference rules that characterise what makes “groups” in music. All musicologists (and many others) are familiar with this reference work that guided years of thinking. In her article, Irène Deliège asks an important question: are these rules perceptually relevant to musicians and non musicians? To answer, she takes two different directions. In a first experiment, she presents natural stimuli (from Bach to Stravinsky) to participants and asks them to segment the pieces. She examines whether listeners’ responses coincide with the rules (e.g., change in register, in dynamic, in articulation, …) or not and quantifies the perceptual relevance of each rule. In a second experiment, she uses synthesized material that was manipulated to contain two conflicting/potential segmentation (i.e., two different rules) at different places and examines which rule the participants are using when asked to segment the music samples. The findings globally validate a large part of the famous Lerdahl and Jackendoff’s rules but this elegant work also paves the way to looking for alternatives. Indeed, all rules are not equally relevant and there are other principles that guide listeners’ segmentation of musical material. 

In the same vein, one can find more recent work about the perceptual relevance and role of melodic accent by [Müllensiefen, Pfleiderer, & Frieler, 2009](https://www.tandfonline.com/doi/abs/10.1080/09298210903085857){:target="_blank”}. In this interesting study, they used a modelling approach to find out what is a good subset of rules (starting with 38 accent descriptors) to predict the accents perceived/reported by the participants when listening to pop music.

When it comes to phrases perception, [Xiangbin Teng](https://sites.google.com/site/xiangbinteng2/){:target="_blank”}, myself, and [David Poeppel](https://en.wikipedia.org/wiki/David_Poeppel){:targets="_blank"} recorded neurophysiological data from participants listening to Bach chorals in their original forms as well as in manipulated versions and found low-frequency neural component that modulated the neural rhythms of beat tracking and reliably parsed musical phrases. in other words, this EEG study shows nicely how our brain tracks phrase boundaries. If interested, check out this great [work](https://www.biorxiv.org/content/biorxiv/early/2021/07/15/2021.07.15.452556.full.pdf){:target="_blank”}! 

***

*Cecchetti, G., Herff, S.A., & Rohrmeier, M.A. (2022) [Musical garden paths: Evidence for syntactic revision beyond the linguistic domain.](https://onlinelibrary.wiley.com/doi/full/10.1111/cogs.13165){:targets="_blank"}  Cognitive Science, 46: e13165. doi: 10.1111/cogs.13165.*

The authors examine how listeners process complex musical syntax and make a parallel with the well studied effects observed in the language domain. Tonal melodies can be simple and unambiguous but they can also be more complex, with several possible options (in terms of which note/harmony follow each other) and one solution has to be chosen by the listener. In the language domain, three steps have been described to process complex syntactic structure: 1) the participant follows one of the possible or prevalent interpretation (referred to as the “ preferred one”); 2) there is a critical event that is unlikely to occur in the preferred interpretation, leading to 3) the participant either reanalyses the previous events or selects one of the many alternatives that were considered in parallel. To test whether the same happens in the music domain, the authors propose short melodies containing an ambiguous event (i.e., a mistuned tone) that is followed by a correct or not-correct ending, depending on the interpretation of the ambiguous event. Results (i.e., changes in “preferred” interpretation of the tonal sequence and congruency between participants’ interpretation of the ambiguous event and the selected interpretation) support that the processing follows the expected 3 steps.

Now, the next challenge is to figure out the nature of the switch. It is tricky because both a serial-processing perspective (see Frazier & Rayner, 1982; Friederici, 1995) and a parallel-processing models (Gibson & Pearlmutter, 2000) can offer an explanation to the garden-path effect observed here.  We are currently focusing on a similar question in the case of speech prosody. We recently showed that a stress early in a sentence can switch meaning (from literal to ironic, see [Research page](https://pauline-lm.github.io/research/){:targets="_blank"}), but we still need to understand the process behind such an effect.

Looking forward to know whether people re-interpret previous events after the “surprise” or already have parallel interpretations and select one of them after the “surprise” when listening to music. We would learn a lot from such findings!
